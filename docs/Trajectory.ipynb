{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad65d2-88e9-4952-87fb-c0e799ba8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase\n",
    "import networkx as nx\n",
    "import os\n",
    "from monty.serialization import loadfn\n",
    "from glob import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import h5py\n",
    "\n",
    "\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from pymatgen.analysis.graphs import MoleculeGraph\n",
    "from pymatgen.analysis.local_env import OpenBabelNN, CovalentBondNN\n",
    "from pymatgen.util.graph_hashing import weisfeiler_lehman_graph_hash\n",
    "\n",
    "from radqm9_pipeline.elements import read_elements\n",
    "from radqm9_pipeline.modules import merge_data, flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f44cd-ce40-4fab-8ece-62444d9e24c9",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18daee-2acc-431a-9784-53913a68966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_dict = read_elements('/pscratch/sd/m/mavaylon/sam_ldrd/radqm9_pipeline/src/radqm9_pipeline/modules/elements.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae1db3a-e2cb-4638-9b0f-30e465f69b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = \"/pscratch/sd/m/mavaylon/radqm9pipeline_cut_paste/update_20240123/Trajectories/trajectories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23592f4f-da4a-4263-b1b8-4c590bf12820",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=merge_data(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c0dd2-5e52-494b-9550-1bc2ed94f73e",
   "metadata": {},
   "source": [
    "# Resolve Split Trjacetories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9faeccbf-7db2-43b8-b132-38fc04a9e717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_filter(data: dict):\n",
    "    \"\"\"\n",
    "    Flatten bucket (dict) to list\n",
    "    \"\"\"\n",
    "    data_to_be_parsed = []\n",
    "    for mol_id in tqdm(data):\n",
    "        for pair in data[mol_id]:\n",
    "            data_to_be_parsed.append(pair)\n",
    "    return data_to_be_parsed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7d537-1b21-46be-bc02-44a2d41b439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_mol_id(data: list):\n",
    "    \"\"\"\n",
    "    Return: \n",
    "    - Good_data--> This means data that either had no duplicates or data that has been able to be mended \n",
    "    - bad_ids --> are events that are not able to be mended\n",
    "    - length is the total number of events both mended and unmended\n",
    "    \n",
    "    TODO: fix the merge duplication in geometries, for now this does not matter since we downsample.\n",
    "  \n",
    "    \"\"\"\n",
    "    bucket = {}\n",
    "    \n",
    "    # Step 1    \n",
    "    \"\"\"\n",
    "    Bucket into mol ids\n",
    "    \"\"\"\n",
    "    for event in tqdm(data):\n",
    "        try:\n",
    "            bucket[event['mol_id']].append(event) \n",
    "        except KeyError:\n",
    "            bucket[event['mol_id']] = [event]\n",
    "    \n",
    "    # Step 2\n",
    "    \"\"\"\n",
    "    Find duplicate pairs in each mol id.\n",
    "    What are duplicate pairs? These are training sessions that most likely continued as a separate job. \n",
    "    They need to be attached if possible. \n",
    "    \"\"\"\n",
    "    length=0\n",
    "    bad_ids = []\n",
    "    for mol_id in tqdm(bucket):\n",
    "        pairs = [event['charge_spin'] for event in bucket[mol_id]]\n",
    "        # Get a list of all charge_spins that have duplicates.\n",
    "        duplicate_pairs = [item for item, count in collections.Counter(pairs).items() if count > 1] \n",
    "        if len(duplicate_pairs)!=0:\n",
    "            \"\"\"\n",
    "            Handle the duplicate pairs to see if they can be merged\n",
    "            \"\"\"\n",
    "            len_p = len(bucket[mol_id])\n",
    "            for dup in duplicate_pairs:\n",
    "                bad_data = []\n",
    "                # Order events\n",
    "                case_events = [event for event in bucket[mol_id] if event['charge_spin']==dup]\n",
    "                bad_events = []\n",
    "                bad_events += case_events\n",
    "\n",
    "                for event in case_events: # remove events to be fixed. If fixable, add back at the end.\n",
    "                    bucket[mol_id].remove(event) \n",
    "        \n",
    "                ordered = [case_events[0]]\n",
    "                del case_events[0]\n",
    "                \n",
    "                counter = 0\n",
    "                threshold = 30\n",
    "                while len(case_events)!=0:\n",
    "                    if len(bad_data)==0:\n",
    "                        for event in case_events:\n",
    "                            beg = event['geometries'][0]\n",
    "                            end = event['geometries'][len(event['geometries'])-1]\n",
    "\n",
    "                            ordered_beg = ordered[0]['geometries'][0]\n",
    "                            ordered_end = ordered[len(ordered)-1]['geometries'][len(ordered[len(ordered)-1]['geometries'])-1]\n",
    "\n",
    "                            if beg==ordered_end:\n",
    "                                ordered.append(event)\n",
    "                                case_events.remove(event)\n",
    "                            elif end==ordered_beg:\n",
    "                                ordered.insert(0, event)\n",
    "                                case_events.remove(event)\n",
    "                            else:\n",
    "                                counter+=1\n",
    "                                if counter>threshold:\n",
    "                                    bad_data.append(mol_id)\n",
    "                                else:\n",
    "                                    continue\n",
    "                    else:\n",
    "                        break\n",
    "                    \n",
    "                if len(bad_data)==0:                \n",
    "                    # Merge the ordered events: forces, geometries\n",
    "                    merged_event = {}\n",
    "                    merged_event['task_id'] = ordered[0]['task_id']\n",
    "                    merged_event['mol_id'] = mol_id\n",
    "                    merged_event['name'] = ordered[0]['name']\n",
    "                    merged_event['charge'] = ordered[0]['charge']\n",
    "                    merged_event['spin'] = ordered[0]['spin']\n",
    "                    merged_event['charge_spin'] = ordered[0]['charge_spin']\n",
    "                    merged_event['species'] = ordered[0]['species']\n",
    "\n",
    "                    geometries = []\n",
    "                    energies = []\n",
    "                    grads = []\n",
    "                    mulliken = []\n",
    "                    resp = []\n",
    "                    dipole_moments = []\n",
    "                    dipole_moments_resp = []\n",
    "                    for event in ordered:\n",
    "                        geometries += event['geometries']\n",
    "                        energies += event['energies']\n",
    "                        grads += event['gradients']\n",
    "                        mulliken += event['mulliken']\n",
    "                        resp += event['resp']\n",
    "                        dipole_moments += event['dipole_moments']\n",
    "                        dipole_moments_resp += event['dipole_moments_resp']\n",
    "\n",
    "                    merged_event['geometries'] = geometries\n",
    "                    merged_event['energies'] = energies\n",
    "                    merged_event['gradients'] = grads\n",
    "                    merged_event['mulliken'] = mulliken\n",
    "                    merged_event['resp'] = resp\n",
    "                    merged_event['dipole_moments'] = dipole_moments\n",
    "                    merged_event['dipole_moments_resp'] = dipole_moments_resp\n",
    "\n",
    "                    bucket[mol_id].append(merged_event)\n",
    "                else:\n",
    "                    bad_ids += bad_events\n",
    "            len_r = len(bucket[mol_id])\n",
    "            length += len_p-len_r\n",
    "    good_data = flatten_filter(bucket)\n",
    "        \n",
    "    return good_data, bad_ids, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96fdd51-8c15-42ad-83b4-b1a2e670b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_data_bucket, b_data_bucket, length = bucket_mol_id(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc8b32-24b7-4955-8606-66189500d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_id(data: list):\n",
    "    for item in data:\n",
    "        item['mol_cs'] = str(item['mol_id']) + str(item['charge_spin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce08ac0-334d-4f9d-ae57-909e743b5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_unique_id(g_data_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb93af6-5ae1-4ddf-ae9d-130900c4087a",
   "metadata": {},
   "source": [
    "# Filter Forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc357500-d054-4f63-91f8-18c6a0d743cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_magnitude_filter(cutoff: float,\n",
    "                           data: list):\n",
    "    \"\"\"\n",
    "    This method returns both data that meets the cuttoff value and data that is equal to or above the cuttoff value.\n",
    "    If this is run before downsampling, it removes the entire data point trajectory.\n",
    "    \n",
    "    Returns: lists\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    bad = []\n",
    "    \n",
    "    for item in tqdm(data):\n",
    "        forces = item['gradients']\n",
    "        for path_point in forces:\n",
    "            next_item = False\n",
    "            for atom in path_point:\n",
    "                res = np.sqrt(sum([i**2 for i in atom]))\n",
    "                if res >= cutoff:\n",
    "                    bad.append(item)\n",
    "                    next_item = True\n",
    "                    break\n",
    "            if next_item:\n",
    "                break\n",
    "        if not next_item:\n",
    "            good.append(item)\n",
    "                            \n",
    "    return good, bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8e2410-7bae-484c-9b04-1ffa3d0f7d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'force_magnitude_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g_data, b_data \u001b[38;5;241m=\u001b[39m \u001b[43mforce_magnitude_filter\u001b[49m(cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m, data\u001b[38;5;241m=\u001b[39mg_data_bucket)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'force_magnitude_filter' is not defined"
     ]
    }
   ],
   "source": [
    "g_data, b_data = force_magnitude_filter(cutoff=10.0, data=g_data_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0477992-0062-4f85-ac1d-8d76355eff73",
   "metadata": {},
   "source": [
    "# Sparse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8c0ee-9319-4b84-acdd-41f1c4ba8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_force_trajectory(pair):\n",
    "    \"\"\"\n",
    "    This method will take a specfic spin charge pair. At each point in the optimization trajectory, the \n",
    "    \"\"\"\n",
    "    forces = {}\n",
    "    for i in range(len(pair['gradients'])):\n",
    "        temp = []\n",
    "        for atom in pair['gradients'][i]:\n",
    "            res = np.sqrt(sum([j**2 for j in atom]))\n",
    "            temp.append(res)\n",
    "        forces[i] = np.mean(temp)\n",
    "    del forces[0]\n",
    "    return forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fff46-250b-4bef-88c6-63826817a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_trajectory(bucket: list):\n",
    "    \"\"\"\n",
    "    This takes the cleaned data and will sparsifiy the optimization trajectories. How this is done will depend on the\n",
    "    charge_spin pair:\n",
    "    - Neutral Singlet (0,1): First and Last\n",
    "    - Other: First, Last, and structure with the highest molecular force other than the First.\n",
    "    \n",
    "    Note: Molecular Force is just the average of the force magnitudes of each atom in the molecule:\n",
    "    \"\"\"\n",
    "    \n",
    "    for pair in tqdm(bucket):\n",
    "        if pair['charge_spin'] == '0,1':\n",
    "            geometries = [pair['geometries'][0], pair['geometries'][-1]]\n",
    "            energies = [pair['energies'][0], pair['energies'][-1]]\n",
    "            grads = [pair['gradients'][0], pair['gradients'][-1]]\n",
    "            mulliken = [pair['mulliken'][0], pair['mulliken'][-1]]\n",
    "            resp = [pair['resp'][0], pair['resp'][-1]]\n",
    "            dipole_moments = [pair['dipole_moments'][0], pair['dipole_moments'][-1]]\n",
    "            dipole_moments_resp = [pair['dipole_moments_resp'][0], pair['dipole_moments_resp'][-1]]\n",
    "\n",
    "            pair['geometries'] = geometries\n",
    "            pair['energies'] = energies\n",
    "            pair['gradients'] = grads\n",
    "            pair['mulliken'] = mulliken\n",
    "            pair['resp'] = resp\n",
    "            pair['dipole_moments'] = dipole_moments\n",
    "            pair['dipole_moments_resp'] = dipole_moments_resp\n",
    "        else:\n",
    "            force_dict = average_force_trajectory(pair)\n",
    "            max_index = max(force_dict, key=force_dict.get)\n",
    "\n",
    "            geometries = [pair['geometries'][0], pair['geometries'][max_index], pair['geometries'][-1]]\n",
    "            energies = [pair['energies'][0], pair['energies'][max_index], pair['energies'][-1]]\n",
    "            grads = [pair['gradients'][0], pair['gradients'][max_index], pair['gradients'][-1]]\n",
    "            mulliken = [pair['mulliken'][0], pair['mulliken'][max_index], pair['mulliken'][-1]]\n",
    "            resp = [pair['resp'][0], pair['resp'][max_index], pair['resp'][-1]]\n",
    "            dipole_moments = [pair['dipole_moments'][0], pair['dipole_moments'][max_index], pair['dipole_moments'][-1]]\n",
    "            dipole_moments_resp = [pair['dipole_moments_resp'][0], pair['dipole_moments_resp'][max_index], pair['dipole_moments_resp'][-1]]\n",
    "\n",
    "            pair['geometries'] = geometries\n",
    "            pair['energies'] = energies\n",
    "            pair['gradients'] = grads\n",
    "            pair['mulliken'] = mulliken\n",
    "            pair['resp'] = resp\n",
    "            pair['dipole_moments'] = dipole_moments\n",
    "            pair['dipole_moments_resp'] = dipole_moments_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c1a33-adab-435c-8bf3-305464f28180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(species, position):\n",
    "    atoms = ase.atoms.Atoms(symbols=species,\n",
    "                            positions=position)\n",
    "    mol = AseAtomsAdaptor.get_molecule(atoms)\n",
    "    graph = MoleculeGraph.with_local_env_strategy(mol, OpenBabelNN())\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb9def-8040-4977-bd62-fe727981d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_broken_graphs(data: list):\n",
    "    broken = []\n",
    "    good = []\n",
    "    \n",
    "    for item in tqdm(data):\n",
    "        for traj_point in item['geometries'][1:]: # Ignore the first is in the filter_broken_graphs\n",
    "            graph = build_graph(item['species'], traj_point)\n",
    "            connected = nx.is_connected(graph.graph.to_undirected())\n",
    "            if not connected:\n",
    "                broken.append(item)\n",
    "                break\n",
    "        if connected:\n",
    "            good.append(item)\n",
    "\n",
    "    return good, broken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477f0d9-c60a-4da3-a74e-697b7d1ea52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_good, broken =filter_broken_graphs(g_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1113ba5-47ca-4c49-abe9-8fa80c0eab79",
   "metadata": {},
   "source": [
    "### Check point clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68856097-8084-4761-8c77-432a1f207a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('/pscratch/sd/m/mavaylon/new_pipe/redone_2024_5_10_full_filtered.h5', 'w') as file:\n",
    "    g1 =file.create_group('clean_data')\n",
    "    raw=[]\n",
    "    for item in tqdm(filtered_good):\n",
    "        raw.append(str(item))\n",
    "    g1.create_dataset('data',data=raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c77f07-24eb-4d1d-acdd-3d631ff93967",
   "metadata": {},
   "source": [
    "### Read Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8648585-67a1-4b39-a0ee-9f061660c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file=h5py.File('/pscratch/sd/m/mavaylon/new_pipe/redone_2024_5_10_full_filtered.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a97ee-03ab-41f7-a0c5-0c8c61b78ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_clean_data=[]\n",
    "for point in tqdm(read_file['clean_data']['data']):\n",
    "    point = ast.literal_eval(point.decode('utf-8'))\n",
    "    merged_all_clean_data.append(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb00a80-d465-408a-9dd4-ed7d55de383c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28754df8-73d9-463a-9369-c09ec5b3d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_weight(data: list):\n",
    "    \"\"\"\n",
    "    The method takes in a list of data (either trajectories or single points) and sorts into a distribution\n",
    "    dictionary. The keys are the species/formula and the value of each key is the weight. appearing number of times the species\n",
    "    appears in the dataset.\n",
    "    \"\"\"\n",
    "    dict_dist = {}\n",
    "    for item in tqdm(data):\n",
    "        species_num = []\n",
    "        species=''.join((sorted(item['species'])))\n",
    "        \n",
    "        for element in item['species']:\n",
    "            species_num.append(elements_dict[element])\n",
    "\n",
    "        species_sum = sum(species_num)\n",
    "        try:\n",
    "            dict_dist[species].append(species_sum)\n",
    "            dict_dist[species] = [dict_dist[species][0]]*len(dict_dist[species])\n",
    "        except KeyError:\n",
    "            dict_dist[species] = [species_sum]\n",
    "        \n",
    "    return dict_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c519da6-ed7e-419b-bc06-7fb174ad26fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def molecule_weight(data: list, weight_dict):\n",
    "    \"\"\"\n",
    "    This method takes in data and assigns the mass.\n",
    "    Python does a weird thing floats e.g., {126.15499999999993, 126.15499999999994}, having this and\n",
    "    get_molecule_weight gurantees that species that are the same are not being assigned different weights.\n",
    "    \"\"\"\n",
    "    for item in tqdm(data):\n",
    "        weight = weight_dict[''.join((sorted(item['species'])))][0]\n",
    "        item['molecule_mass'] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66870e-b471-4fe0-97a8-508d2d03fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dist = get_molecule_weight(cleaned)\n",
    "molecule_weight(cleaned, merged_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49756c5c-f940-46ab-9f46-f6b41b9973db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_to_data(data: list):\n",
    "    \"\"\"\n",
    "    This method buckets the data by the mass such that the dict key is the mass and the values are the data\n",
    "    points.\n",
    "    \"\"\"\n",
    "    dict_data = {}\n",
    "    for item in tqdm(data):\n",
    "        try:\n",
    "            dict_data[item['molecule_mass']].append(item)\n",
    "        except KeyError:\n",
    "            dict_data[item['molecule_mass']] = [item]\n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6673af-d2f4-48dc-8c24-629f304a74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtd = weight_to_data(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb8526-9915-45d1-80f6-ff8969805f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_dict(data: dict):\n",
    "    \"\"\"\n",
    "    This method takes in the output of weight_to_data and returns a dictionary that is sorted from largest\n",
    "    to smallest mass. The keys are the mass and the values are the number of appearances.\n",
    "    \"\"\"\n",
    "    length_dict = {key: len(value) for key, value in data.items()}\n",
    "    sorted_length_dict = {k: length_dict[k] for k in sorted(length_dict, reverse=True)}\n",
    "    \n",
    "    return sorted_length_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf17c92-6961-45d9-81a2-88da0b16e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sld = length_dict(wtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66dba8a-3aa7-4147-bdea-923c14ca5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: make into a function\n",
    "\n",
    "The split method is as follows:\n",
    "I have a dictionary that is sorted from highest mass to lowest mass. The value of each key is the number of times\n",
    "that mass is in the data. Another way to think of this, is the number of trajectories or SPs that have that\n",
    "mass. \n",
    "\n",
    "We have a list for each split that stores the the masses.\n",
    "We have three variables that store the size of the splits. \n",
    "\n",
    "Each iteration will add a mass to a split. This ensures that the mass is in one split. This means that that species\n",
    "is only in that split. \n",
    "\n",
    "It will continue to add until all the masses have been added. \n",
    "\"\"\"\n",
    "\n",
    "# Take initial points (the highest masses) and have them in the data\n",
    "train_mass = [152.037]\n",
    "test_mass = [144.09200000000007]\n",
    "val_mass = [143.1080000000001]\n",
    "\n",
    "train = sld[152.037] # trackers for dataset sizes\n",
    "test = sld[144.09200000000007]\n",
    "val = sld[143.1080000000001]\n",
    "\n",
    "sld.pop(152.037)\n",
    "sld.pop(144.09200000000007)\n",
    "sld.pop(143.1080000000001)\n",
    "\n",
    "# Sort the data \n",
    "# data is a dict: mass-># of trajs\n",
    "for mass in sorted_length_dict:\n",
    "    temp_total = train+val+test\n",
    "    train_ratio = .65-(train/temp_total)\n",
    "    test_ratio = .25-(test/temp_total)\n",
    "    val_ratio = .1-(val/temp_total)\n",
    "    \n",
    "    if train_ratio > val_ratio and train_ratio>test_ratio:\n",
    "        train_mass.append(mass)\n",
    "        train += sorted_length_dict[mass]\n",
    "    if val_ratio > train_ratio and val_ratio>test_ratio:\n",
    "        val_mass.append(mass)\n",
    "        val += sorted_length_dict[mass]\n",
    "    if test_ratio > val_ratio and test_ratio>train_ratio:\n",
    "        test_mass.append(mass)\n",
    "        test += sorted_length_dict[mass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97e3ea-ff02-4bb6-8d8c-c5561623f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "train/(train+val+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f370059-173a-439e-8f99-6997783a10cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mval\u001b[49m\u001b[38;5;241m/\u001b[39m(train\u001b[38;5;241m+\u001b[39mval\u001b[38;5;241m+\u001b[39mtest)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "val/(train+val+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbb384-b8a2-4e70-ad28-51e30647ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test/(train+val+test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb6874-8251-4d6f-b7a2-fe42ad62c61b",
   "metadata": {},
   "source": [
    "### Visualize the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa30fa0-a4a7-48d6-9122-197277c73dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sld = length_dict(wtd) # you need to call this again yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a0e04-0cd1-4d76-979b-b3bb4d423fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset={key: sorted_length_dict[key] for key in train_mass if key in sorted_length_dict}\n",
    "test_subset={key: sorted_length_dict[key] for key in test_mass if key in sorted_length_dict}\n",
    "val_subset={key: sorted_length_dict[key] for key in val_mass if key in sorted_length_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cab94-2998-4d04-8dda-0281c1d2a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp=[[x]*train_subset[x] for x in train_subset]\n",
    "test_temp=[[x]*test_subset[x] for x in test_subset]\n",
    "val_temp=[[x]*val_subset[x] for x in val_subset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd632d-a92d-4cf3-aea0-63d34eb66726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "train_subset_merged = list(chain.from_iterable(train_temp))\n",
    "test_subset_merged = list(chain.from_iterable(test_temp))\n",
    "val_subset_merged = list(chain.from_iterable(val_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16298e90-8d33-4475-ae6b-7f6db39d1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_subset_merged, bins=50)\n",
    "plt.ylabel('Frequency (log)')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Molecule Mass')\n",
    "plt.title('Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49bd33-98cc-4eaf-b5d3-98a07f3b213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_subset_merged, bins=50)\n",
    "plt.ylabel('Frequency (log)')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Molecule Mass')\n",
    "plt.title('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea8797-72e7-4048-ad6b-4883b82f032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(val_subset_merged, bins=50)\n",
    "plt.ylabel('Frequency (log)')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Molecule Mass')\n",
    "plt.title('Val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d45edb-90c6-4946-b131-80786daa7f43",
   "metadata": {},
   "source": [
    "### Make Manual Switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac47bd-ef3d-4a64-9383-6082de656004",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch=[45.04100000000001,\n",
    "54.09200000000001,\n",
    "60.05600000000001,\n",
    "70.05099999999999,\n",
    "72.10699999999997,\n",
    "76.05799999999999,\n",
    "78.06999999999998,\n",
    "84.08199999999998,\n",
    "85.06599999999997,\n",
    "87.08199999999998,\n",
    "88.06199999999998,\n",
    "93.08899999999998,\n",
    "95.10499999999999,\n",
    "97.11699999999996,\n",
    "98.06099999999999,\n",
    "102.09299999999996,\n",
    "102.17699999999992,\n",
    "107.06799999999998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b3b05-ad91-4147-b2f0-e62ac69b6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mass in switch:\n",
    "    val_mass.append(mass)\n",
    "    val += sorted_length_dict[mass]\n",
    "    \n",
    "    test_mass.remove(mass)\n",
    "    test -= sorted_length_dict[mass]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d2c4b-883b-40ff-8c4c-03ed51e00adc",
   "metadata": {},
   "source": [
    "This is is to ensure even mass disitrubtion across the sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c671386-9f4a-4daf-b59c-0a118b911c7b",
   "metadata": {},
   "source": [
    "# Resolve splits to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1241261-ccfd-4bfd-b5d2-d561aded50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [wtd[x] for x in train_mass]\n",
    "train_data = list(chain.from_iterable(train_data))\n",
    "\n",
    "val_data = [wtd[x] for x in val_mass]\n",
    "val_data = list(chain.from_iterable(val_data))\n",
    "\n",
    "test_data = [wtd[x] for x in test_mass]\n",
    "test_data = list(chain.from_iterable(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc861369-f3f2-4547-bba1-8b78ae7f4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train':train_data,\n",
    "        'val': val_data,\n",
    "        'test': test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30093e25-a268-4bf5-93bc-059fec00784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_minimal_atoms(data: dict,\n",
    "                energy: str = None,\n",
    "                forces: str = None,\n",
    "                charge:str = None,\n",
    "                spin:str = None,\n",
    "                train = False) -> ase.Atoms:\n",
    "    \"\"\" \n",
    "    Populate Atoms class with atoms in molecule.\n",
    "        atoms.info : global variables\n",
    "        atoms.array : variables for individual atoms\n",
    "        \n",
    "    Both \"energy\" and \"forces\" are the dict strings in data.\n",
    "    \"\"\"\n",
    "    atom_list = []\n",
    "    for i in range(len(data['geometries'])):\n",
    "        atoms = ase.atoms.Atoms(\n",
    "            symbols=data['species'],\n",
    "            positions=data['geometries'][i]\n",
    "        )\n",
    "        if energy is not None:\n",
    "            atoms.info['energy'] = data[energy][i]\n",
    "        if forces is not None:\n",
    "            atoms.arrays['forces'] = np.array(data[forces][i])\n",
    "        if charge is not None:\n",
    "             atoms.info['charge'] = data[charge]\n",
    "        if spin is not None:\n",
    "            atoms.info['spin'] = data[spin]\n",
    "\n",
    "        if i == 0:\n",
    "            atoms.info['position_type'] = 'start'\n",
    "        if i == 1:\n",
    "            if data['charge_spin'] == '0,1':\n",
    "                atoms.info['position_type'] = 'end'\n",
    "            else:\n",
    "                atoms.info['position_type'] = 'middle'\n",
    "        if i == 2:\n",
    "            atoms.info['position_type'] = 'end'\n",
    "        atom_list.append(atoms)\n",
    "    return atom_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c2b9e-de84-4a1b-bbf8-edc0a4e50cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_minimal_atoms_iterator(data: list,\n",
    "                         train=False):\n",
    "    \"\"\"\n",
    "    This method assumes the data has been validated. This will create ASE atoms to be written.\n",
    "    \n",
    "    The input needs to be a list of lists that contain the event dictionaries. Each inner list needs to represent all the events for a single\n",
    "    mol_id.\n",
    "    \"\"\"\n",
    "    data_set=[]\n",
    "    for point in tqdm(data):\n",
    "        atoms=build_minimal_atoms(point, energy='energies', forces='gradients', charge='charge', spin='spin', train=train)\n",
    "        data_set+=atoms\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfa913-9216-4930-b004-94e2e1cd4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "build = {}\n",
    "for split in data:\n",
    "    if split == 'train':\n",
    "        build[split] = build_minimal_atoms_iterator(data[split], train=True)\n",
    "    else:\n",
    "        build[split] = build_minimal_atoms_iterator(data[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a28f79-7142-4188-968d-5724f79dc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data: dict,\n",
    "                   file_name:str,\n",
    "                   path:str):\n",
    "    \"\"\"\n",
    "    This method will handle the I/O for writing the data to xyz files to the path provided.\n",
    "    \"\"\"\n",
    "    train_data = data['train']\n",
    "    val_data = data['val']\n",
    "    test_data = data['test']\n",
    "    \n",
    "    train_file = os.path.join(path,file_name+'_train.xyz')\n",
    "    ase.io.write(train_file, train_data,format=\"extxyz\")\n",
    "     \n",
    "    val_file = os.path.join(path,file_name+'_val.xyz')\n",
    "    ase.io.write(val_file, val_data,format=\"extxyz\")\n",
    "    \n",
    "    test_file = os.path.join(path,file_name+'_test.xyz')\n",
    "    ase.io.write(test_file, test_data,format=\"extxyz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2c56f-fcb4-4dfb-aeef-a04faf7ce016",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path ='/pscratch/sd/m/mavaylon/chem_final_data/Traj/Full_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b33670-5ae8-4eca-9a16-402a2697bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_data_path ='/pscratch/sd/m/mavaylon/chem_final_data/Traj/Charge_data/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
